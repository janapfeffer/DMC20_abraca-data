{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "# import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "from sklearn.externals import joblib \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.interpolate import interpn\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_means =pd.read_csv('./data/sales_means.csv', sep='|').drop(columns=[\"predicted_promotion\"])\n",
    "scaler = joblib.load('scaler.pkl') \n",
    "df_test = pd.read_csv('data/orders0206_test.csv', sep='|', parse_dates=['time'])\n",
    "df_test['date'] = [d.date() for d in df_test['time']]\n",
    "df_train = pd.read_csv('data/orders0206_train.csv', sep='|', parse_dates=['time'])\n",
    "df_train['date'] = [d.date() for d in df_train['time']]\n",
    "bundles = pd.read_csv('./data/bundles.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_means.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_means = sales_means.join(bundles.set_index(\"itemID\"), on=\"itemID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_means.sample(frac=1).reset_index(drop=True)\n",
    "X_train = sales_means.tail(len(sales_means)-2000)#[(sales_means['weekGroup']<=9)]# & (sales_means['weekGroup']==10)]\n",
    "X_cv =  sales_means.head(2000)#[sales_means['weekGroup']==10]\n",
    "X_test = sales_means[sales_means['weekGroup']==11]\n",
    "Y_train = X_train['count']\n",
    "Y_cv = X_cv['count']\n",
    "Y_test = X_test['count']\n",
    "del X_train['count']\n",
    "del X_cv['count']\n",
    "del X_test['count']\n",
    "del X_train['weekGroup']\n",
    "del X_cv['weekGroup']\n",
    "del X_test['weekGroup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train['itemID']\n",
    "del X_cv['itemID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "model = XGBRegressor(\n",
    "    booster = 'gbtree', \n",
    "    colsample_bytree = 0.8, \n",
    "    gamma = 0, \n",
    "    learning_rate = 0.08, \n",
    "    max_depth= 3, \n",
    "    min_child_weight= 1, \n",
    "    n_estimators= 200, \n",
    "    objective= 'reg:logistic', \n",
    "    seed= 20, \n",
    "    silent= 1, \n",
    "    subsample= 0.8)\n",
    "model.fit(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    eval_metric=\"rmse\", \n",
    "    eval_set=[(X_train, Y_train), (X_cv, Y_cv)], \n",
    "    verbose=True, \n",
    "    early_stopping_rounds = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly_express as px\n",
    "columns = X_train.columns\n",
    "feature_importances = pd.DataFrame({'columns': columns,'importance':model.feature_importances_})\n",
    "feature_importances = feature_importances.sort_values(by='importance',ascending=False)\n",
    "px.bar(feature_importances,x='columns',y='importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_result(y: dict, y_pred: dict):\n",
    "    monetary_value = 0\n",
    "    y_pred = defaultdict(int, y_pred)  # return prediction of 0 for items without prediction\n",
    "    \n",
    "    for item in set(y_pred).difference(set(y)):\n",
    "        y[item] = 0  # make sure that all items for which a demand has been predicted are contained in the actual demands\n",
    "    \n",
    "    for item, demand in y.items():\n",
    "        predicted_demand = y_pred[item]\n",
    "        price = product_prices[item]\n",
    "        monetary_value += price * min(demand, predicted_demand)\n",
    "        if predicted_demand > demand:\n",
    "            monetary_value -= .6 * price * (predicted_demand - demand)\n",
    "            \n",
    "    return monetary_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = pd.read_csv('data/infos.csv', sep='|', index_col='itemID')\n",
    "df_items = pd.read_csv('data/items.csv', sep='|', index_col='itemID')\n",
    "df_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual demand\n",
    "y = df_test.groupby(by='itemID')['order'].sum().to_dict()\n",
    "\n",
    "# baseline 1 (average demand of previous 14 days)\n",
    "y_baseline1 = df_train[df_train['time'] >= '2018-05-19'].groupby(by='itemID')['order'].sum().to_dict()\n",
    "\n",
    "# baseline 2 (average demand of previous half year)\n",
    "total_orders = df_train.groupby(by='itemID')['order'].sum().to_dict()\n",
    "total_observed_days = (df_train['time'].dt.normalize().max() - df_train['time'].dt.normalize().min()).days\n",
    "y_baseline2 = {item: orders / total_observed_days * 14 for item, orders in total_orders.items()}  # 14-day avg. demand\n",
    "\n",
    "df_info = pd.read_csv('data/infos.csv', sep='|', index_col='itemID')\n",
    "df_items = pd.read_csv('data/items.csv', sep='|', index_col='itemID')\n",
    "product_prices = df_info['simulationPrice'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to unseen data\n",
    "y_xgboost_all = dict()\n",
    "for prod in X_test.itemID.unique():\n",
    "    train_predict = pd.DataFrame(scaler.inverse_transform(pd.DataFrame(model.predict(X_test[X_test.itemID == prod].drop(['itemID'], axis=1))))).rename(columns={0:'predicted_count'})\n",
    "    # train_predict[\"actual_count\"] = test[test.itemID == prod][\"order\"].sum()\n",
    "    y_xgboost_all[prod] = int(train_predict[\"predicted_count\"].sum().round())\n",
    "#     if prod in df_train.itemID.unique():\n",
    "#         y_xgboost_all[prod] = int(train_predict[\"predicted_count\"].sum().round())\n",
    "#     else:\n",
    "#         y_xgboost_all[prod] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perfect result\n",
    "print(f'Perfect Result: {evaluate_result(y, y):.2f}')\n",
    "\n",
    "# baseline 1\n",
    "print(f'Baseline 1: {evaluate_result(y, y_baseline1):.2f}')\n",
    "\n",
    "# baseline 2\n",
    "print(f'Baseline 2: {evaluate_result(y, y_baseline2):.2f}')\n",
    "\n",
    "# random forest\n",
    "print(f'XGBoost: {evaluate_result(y, y_xgboost_all):.2f}')\n",
    "# 953796.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
